# AI Text Generation Detector

A sophisticated machine learning system for detecting AI-generated text using advanced natural language processing techniques.

## ğŸš€ Features

- **Advanced Feature Engineering**: Combines word-level and character-level n-gram features
- **High Accuracy**: SVM model optimized to achieve 99%+ accuracy
- **Comprehensive Preprocessing**: Advanced text cleaning and normalization
- **Production Ready**: Includes training, evaluation, and prediction scripts
- **Modular Architecture**: Well-organized codebase for easy maintenance and extension

## ğŸ“ Project Structure

```
SDP_DRAFT/
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ raw/                   # Raw input data files
â”‚   â”œâ”€â”€ processed/             # Preprocessed/cleaned data
â”‚   â””â”€â”€ combined/              # Combined datasets for training
â”‚
â”œâ”€â”€ preprocessing/             # Data preprocessing scripts
â”‚   â”œâ”€â”€ preprocess.py          # Unified preprocessing script
â”‚   â””â”€â”€ advanced_preprocessing.py  # Advanced preprocessing with extra features
â”‚
â”œâ”€â”€ models/                    # Machine learning models
â”‚   â”œâ”€â”€ train_model.py         # Main training script
â”‚   â”œâ”€â”€ evaluate.py            # Model evaluation
â”‚   â”œâ”€â”€ predict.py             # Prediction/inference
â”‚   â””â”€â”€ saved_models/          # Trained model files
â”‚       â””â”€â”€ README.md          # Model documentation
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ combine_datasets.py    # Combine human/AI datasets
â”‚   â”œâ”€â”€ merge_data.py          # Merge multiple data files
â”‚   â”œâ”€â”€ check_training.py      # Check training status
â”‚   â””â”€â”€ README.md              # Scripts documentation
â”‚
â”œâ”€â”€ scraping/                  # Web scraping utilities
â”‚   â”œâ”€â”€ scraper.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ generation/                # AI content generation
â”‚   â””â”€â”€ ai_writer.py
â”‚
â”œâ”€â”€ utils/                     # General utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ text_utils.py
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ exploration.ipynb      # Data exploration
â”‚
â”œâ”€â”€ examples/                  # Example files
â”‚   â”œâ”€â”€ example_data.json      # Sample data format
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System architecture
â”‚   â””â”€â”€ CONTRIBUTING.md        # Contribution guidelines
â”‚
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ .gitignore                # Git ignore rules
```

## ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/SDP_DRAFT.git
   cd SDP_DRAFT
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“– Usage

### Training the Model

Train the AI text detection model:

```bash
python models/train_model.py
```

The script will:
- Load and balance the dataset
- Create word and character n-gram features
- Train SVM and Logistic Regression models
- Save models to `models/saved_models/`

### Evaluating the Model

Evaluate the trained model:

```bash
python models/evaluate.py
```

### Making Predictions

#### Interactive Mode
```bash
python models/predict.py
```

#### Batch Prediction
```bash
python models/predict.py input.json output.json
```

### Preprocessing Data

Preprocess raw data files:

**Simple preprocessing:**
```bash
python preprocessing/preprocess.py data/raw/ai.json data/processed/ai_clean.json
python preprocessing/preprocess.py data/raw/human.json data/processed/human_clean.json
```

**Advanced preprocessing (with extra cleaning):**
```bash
python preprocessing/advanced_preprocessing.py data/raw/dataset.json data/processed/dataset_clean.json
```

### Utility Scripts

Combine datasets:
```bash
python scripts/combine_datasets.py data/processed/human_clean.json data/processed/ai_clean.json data/combined/combined_dataset.json
```

Check training status:
```bash
python scripts/check_training.py
```

## ğŸ—ï¸ Model Architecture

The model uses:
- **Word-level TF-IDF**: Unigrams, bigrams, and trigrams (25,000 features)
- **Character-level TF-IDF**: 3-6 character n-grams (35,000 features)
- **SVM Classifier**: Linear kernel with optimized hyperparameters
- **Total Features**: 60,000 combined features

For more details, see [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)

## ğŸ“Š Performance

The model achieves:
- **Accuracy**: 99%+
- **Precision**: High precision for both classes
- **Recall**: High recall for both classes
- **F1-Score**: Balanced performance

## ğŸ“ Data Format

Input data should be in JSON format:

```json
[
  {
    "content": "Text content here...",
    "label": "human"  // or "ai"
  }
]
```

See [examples/example_data.json](examples/example_data.json) for a sample file.

## ğŸ¤ Contributing

Contributions are welcome! Please read [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Additional Documentation

- [Architecture Overview](docs/ARCHITECTURE.md)
- [Contributing Guidelines](docs/CONTRIBUTING.md)
- [Data Directory](data/README.md)
- [Saved Models](models/saved_models/README.md)
- [Utility Scripts](scripts/README.md)

## ğŸ” Project Status

This project is actively maintained and ready for use. For issues or questions, please open an issue on GitHub.
